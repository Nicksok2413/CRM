x-django-build: &django-build
  build:
    context: .  # Контекст сборки - корень проекта
    dockerfile: docker/django/Dockerfile  # Путь к Dockerfile для django

services:
  db:
    image: postgres:17-alpine
    container_name: crm_db
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - postgres_data:/var/lib/postgresql/data/ # Монтируем том для БД
    healthcheck:
      # Проверяет, готова ли БД принимать соединения (экранируем $ для shell).
      test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s  # Даем время на запуск Postgres перед первой проверкой
    restart: unless-stopped
    networks:
      - crm_network

  redis:
    image: redis:7-alpine
    container_name: crm_redis
    # Команда для персистентности (сохранять на диск каждые 60с при 1 изменении) и логирования.
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis_data:/data  # Монтируем том для сохранения dump.rdb
    healthcheck:
      # Проверяет, отвечает ли Redis на команду PING.
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 5s  # Даем время на запуск Redis перед первой проверкой
    restart: unless-stopped
    networks:
      - crm_network

  clamav:
    image: clamav/clamav:latest
    container_name: crm_clamav
    user: clamav  # Запускаем от непривилегированного пользователя clamav
    volumes:
      - clamav_data:/var/lib/clamav  # Монтируем том для хранения скачанных антивирусных баз
    restart: unless-stopped
    networks:
      - crm_network

  django:
    <<: *django-build  # Используем якорь
    container_name: crm_django_web
    # Команда, которая будет передана в `exec "$@"` в entrypoint.sh.
    command: gunicorn config.wsgi:application -c /app/docker/django/gunicorn.conf.py
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - log_data:/app/logs  # Монтируем том для логов
      - media_data:/app/uploads  # Монтируем том для медиа-файлов
      - static_data:/app/staticfiles  # Монтируем том для статики
    expose:
      - 8000  # Открываем порт только внутри сети Docker
    depends_on:
      db:
        # Запускаем только после того, как db станет healthy.
        condition: service_healthy
      redis:
        # Запускаем только после того, как redis станет healthy.
        condition: service_healthy
      clamav:
        # Для clamav достаточно дождаться запуска.
        condition: service_started
    restart: unless-stopped
    networks:
      - crm_network

  celery_worker:
    <<: *django-build  # Используем якорь
    container_name: crm_celery_worker
    # Используем entrypoint для worker'а.
    entrypoint: /docker/celery/worker/entrypoint.sh
    # Переопределяем команду, которую запустит entrypoint.
    command: celery -A config.celery worker --loglevel=info
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - log_data:/app/logs  # Монтируем том для логов
    depends_on:
      redis:
        # Запускаем только после того, как redis станет healthy.
        condition: service_healthy
      db:
        # Worker'у нужен доступ к БД для работы с моделями.
        condition: service_healthy
    restart: unless-stopped
    networks:
      - crm_network

  celery_beat:
    <<: *django-build  # Используем якорь
    container_name: crm_celery_beat
    # Используем entrypoint для планировщика.
    entrypoint: /docker/celery/beat/entrypoint.sh
    # Переопределяем команду, которую запустит entrypoint (с указанием pid-файла).
    command: celery -A config.celery beat --loglevel=info --pidfile=/app/celerybeat.pid
    env_file: .env  # Передаем все переменные из .env в контейнер
    volumes:
      - log_data:/app/logs  # Монтируем том для логов
    depends_on:
      redis:
        # Запускаем только после того, как redis станет healthy.
        condition: service_healthy
    restart: unless-stopped
    networks:
      - crm_network

  nginx:
    build:
      context: .  # Контекст сборки - корень проекта
      dockerfile: docker/nginx/Dockerfile  # Путь к Dockerfile для nginx
    container_name: crm_nginx
    volumes:
      - certs_data:/etc/nginx/certs  # Монтируем том для хранения сгенерированных сертификатов
      - media_data:/app/uploads:ro  # Монтируем том для медиа-файлов (только для чтения)
      - nginx_log_data:/var/log/nginx  # Монтируем том для логов Nginx
      - static_data:/app/staticfiles:ro  # Монтируем том для статики (только для чтения)
    # Пробрасываем порты из контейнера на хост-машину, делая сайт доступным извне.
    ports:
      - "80:80"   # HTTP
      - "443:443" # HTTPS
    depends_on:
      - django  # Запускаем Nginx после django
    restart: unless-stopped
    networks:
      - crm_network

  prometheus:
    image: prom/prometheus:latest
    container_name: crm_prometheus
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro  # Монтируем конфиг (только для чтения)
      - prometheus_data:/prometheus  # Монтируем том для хранения собранных метрик
    # Передаем путь к конфигу.
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    ports:
      - "9090:9090"  # Пробрасываем порт для доступа к UI Prometheus
    depends_on:
      - django  # Запускаем Prometheus после django
    restart: unless-stopped
    networks:
      - crm_network

  grafana:
    image: grafana/grafana-oss:latest
    container_name: crm_grafana
    volumes:
      - grafana_data:/var/lib/grafana  # Монтируем том для хранения данных Grafana (настройки, пользователи, дашборды)
      - ./docker/grafana/provisioning/:/etc/grafana/provisioning/:ro  # Монтируем provisioning для автонастройки при старте (только для чтения)
      - ./docker/grafana/dashboards/:/var/lib/grafana/dashboards/:ro  # Монтируем директорию с JSON дашбордами (только для чтения)
    environment:
      # Устанавливаем логин/пароль для админа Grafana.
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"  # Отключить регистрацию новых пользователей
    ports:
      - "3000:3000" # Пробрасываем порт для доступа к UI Grafana
    depends_on:
      - prometheus  # Запускаем Grafana после Prometheus
    restart: unless-stopped
    networks:
      - crm_network

# Определяем все тома для хранения персистентных данных.
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  clamav_data:
    driver: local
  log_data:
    driver: local
  media_data:
    driver: local
  static_data:
    driver: local
  certs_data:
    driver: local
  nginx_log_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# Определяем общую сеть для всех сервисов.
# Это обеспечивает изоляцию и позволяет контейнерам общаться друг с другом по именам.
networks:
  crm_network:
    driver: bridge